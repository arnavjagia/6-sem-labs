{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(6, 6)\n",
    "img = img.unsqueeze(dim = 0).unsqueeze(dim = 0)\n",
    "kernel = torch.ones(3, 3)\n",
    "kernel  = kernel.unsqueeze(dim = 0).unsqueeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Conv2d(in_channels= 1, out_channels= 3, kernel_size= 3, stride= 1, padding= 0)\n",
    "output = layer(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[ 0.5213,  0.4216,  0.7662,  0.5754],\n",
      "          [ 0.1986,  0.2131,  0.1242,  0.2211],\n",
      "          [ 0.1049,  0.2385,  0.0222,  0.2087],\n",
      "          [ 0.3975,  0.3670,  0.5266,  0.4649]],\n",
      "\n",
      "         [[-0.0597, -0.1193,  0.1180, -0.1800],\n",
      "          [-0.0986,  0.1540, -0.0432,  0.0194],\n",
      "          [ 0.0455,  0.0550, -0.0252,  0.0028],\n",
      "          [-0.0086, -0.0812,  0.0661, -0.0954]],\n",
      "\n",
      "         [[ 0.4928,  0.6244,  0.3894,  0.4200],\n",
      "          [ 0.7143,  0.4756,  0.4946,  0.4101],\n",
      "          [ 0.6192,  0.6047,  0.7090,  0.5944],\n",
      "          [ 0.4665,  0.6342,  0.4814,  0.7979]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "kernel = torch.ones(size=(3, 1, 3, 3))\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[4.1842, 4.4336, 4.7258, 4.0594],\n",
      "          [5.0561, 5.5894, 6.2230, 5.7845],\n",
      "          [4.7459, 5.7360, 6.5182, 6.4048],\n",
      "          [4.1200, 5.1612, 4.8040, 5.2231]],\n",
      "\n",
      "         [[4.1842, 4.4336, 4.7258, 4.0594],\n",
      "          [5.0561, 5.5894, 6.2230, 5.7845],\n",
      "          [4.7459, 5.7360, 6.5182, 6.4048],\n",
      "          [4.1200, 5.1612, 4.8040, 5.2231]],\n",
      "\n",
      "         [[4.1842, 4.4336, 4.7258, 4.0594],\n",
      "          [5.0561, 5.5894, 6.2230, 5.7845],\n",
      "          [4.7459, 5.7360, 6.5182, 6.4048],\n",
      "          [4.1200, 5.1612, 4.8040, 5.2231]]]]) torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "output = F.conv2d(img, kernel, stride= 1, padding= 0)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
